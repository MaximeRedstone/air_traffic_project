{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import os, sys\n",
    "\n",
    "# sys.path.append(\"~/X/PythonForDataScience/air_traffic_project/submissions/first_real_submission/\")\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from problem import *\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import geopy.distance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from model_selection_python import *\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "from six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import os\n",
    "\n",
    "from estimator import _merge_external_data\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_merged 1 =  (8902, 20)\n",
      "X_merged 3 =  (8902, 49)\n",
      "X_merged 4 =  (8902, 78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hugo/Desktop/X-HEC/Polytechnique/Semestre 1/Python_For_Data_Science/air_traffic_project/src/estimator.py:38: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_merged 1 =  (2226, 20)\n",
      "X_merged 3 =  (2226, 49)\n",
      "X_merged 4 =  (2226, 78)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_train_data('..')\n",
    "X_test, y_test = get_test_data('..')\n",
    "\n",
    "X_train_merged = _merge_external_data(X_train)\n",
    "X_test_merged = _merge_external_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train_merged = X_train_merged.copy()\n",
    "Xy_train_merged['Passengers'] = y_train\n",
    "Xy_train_merged.to_csv('merged_xy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8902 entries, 0 to 8901\n",
      "Data columns (total 77 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   Departure                       8902 non-null   object \n",
      " 1   Arrival                         8902 non-null   object \n",
      " 2   WeeksToDeparture                8902 non-null   float64\n",
      " 3   std_wtd                         8902 non-null   float64\n",
      " 4   Days_to_departure               8902 non-null   float64\n",
      " 5   route_mean                      8902 non-null   float64\n",
      " 6   year_departure                  8902 non-null   int64  \n",
      " 7   day_departure                   8902 non-null   int64  \n",
      " 8   n_days_departure                8902 non-null   int64  \n",
      " 9   day_mean                        8902 non-null   float64\n",
      " 10  week_mean                       8902 non-null   float64\n",
      " 11  month_mean                      8902 non-null   float64\n",
      " 12  oil_stock_price                 8902 non-null   float64\n",
      " 13  oil_stock_volume                8902 non-null   float64\n",
      " 14  AAL_stock_price                 8902 non-null   float64\n",
      " 15  AAL_stock_volume                8902 non-null   float64\n",
      " 16  SP_stock_price                  8902 non-null   float64\n",
      " 17  SP_stock_volume                 8902 non-null   float64\n",
      " 18  Max TemperatureC_dep            8902 non-null   float64\n",
      " 19  Mean TemperatureC_dep           8902 non-null   float64\n",
      " 20  Min TemperatureC_dep            8902 non-null   float64\n",
      " 21  Dew PointC_dep                  8902 non-null   float64\n",
      " 22  MeanDew PointC_dep              8902 non-null   float64\n",
      " 23  Min DewpointC_dep               8902 non-null   float64\n",
      " 24  Max Humidity_dep                8902 non-null   float64\n",
      " 25  Mean Humidity_dep               8902 non-null   float64\n",
      " 26  Min Humidity_dep                8902 non-null   float64\n",
      " 27  Max Sea Level PressurehPa_dep   8902 non-null   float64\n",
      " 28  Mean Sea Level PressurehPa_dep  8902 non-null   float64\n",
      " 29  Min Sea Level PressurehPa_dep   8902 non-null   float64\n",
      " 30  Max VisibilityKm_dep            8902 non-null   float64\n",
      " 31  Mean VisibilityKm_dep           8902 non-null   float64\n",
      " 32  Min VisibilitykM_dep            8902 non-null   float64\n",
      " 33  Max Wind SpeedKm/h_dep          8902 non-null   float64\n",
      " 34  Mean Wind SpeedKm/h_dep         8902 non-null   float64\n",
      " 35  CloudCover_dep                  8902 non-null   float64\n",
      " 36  WindDirDegrees_dep              8902 non-null   float64\n",
      " 37  LoadFactorDomestic_dep          8902 non-null   float64\n",
      " 38  PassengersDomestic_dep          8902 non-null   int64  \n",
      " 39  latitude_deg_dep                8902 non-null   float64\n",
      " 40  longitude_deg_dep               8902 non-null   float64\n",
      " 41  pop2010_dep                     8902 non-null   float64\n",
      " 42  UnemploymentRate_dep            8902 non-null   float64\n",
      " 43  holidays_dep                    8902 non-null   object \n",
      " 44  GDP_per_cap_dep                 8902 non-null   float64\n",
      " 45  closest_holidays_dep            8902 non-null   float64\n",
      " 46  Max TemperatureC_arr            8902 non-null   float64\n",
      " 47  Mean TemperatureC_arr           8902 non-null   float64\n",
      " 48  Min TemperatureC_arr            8902 non-null   float64\n",
      " 49  Dew PointC_arr                  8902 non-null   float64\n",
      " 50  MeanDew PointC_arr              8902 non-null   float64\n",
      " 51  Min DewpointC_arr               8902 non-null   float64\n",
      " 52  Max Humidity_arr                8902 non-null   float64\n",
      " 53  Mean Humidity_arr               8902 non-null   float64\n",
      " 54  Min Humidity_arr                8902 non-null   float64\n",
      " 55  Max Sea Level PressurehPa_arr   8902 non-null   float64\n",
      " 56  Mean Sea Level PressurehPa_arr  8902 non-null   float64\n",
      " 57  Min Sea Level PressurehPa_arr   8902 non-null   float64\n",
      " 58  Max VisibilityKm_arr            8902 non-null   float64\n",
      " 59  Mean VisibilityKm_arr           8902 non-null   float64\n",
      " 60  Min VisibilitykM_arr            8902 non-null   float64\n",
      " 61  Max Wind SpeedKm/h_arr          8902 non-null   float64\n",
      " 62  Mean Wind SpeedKm/h_arr         8902 non-null   float64\n",
      " 63  CloudCover_arr                  8902 non-null   float64\n",
      " 64  WindDirDegrees_arr              8902 non-null   float64\n",
      " 65  LoadFactorDomestic_arr          8902 non-null   float64\n",
      " 66  PassengersDomestic_arr          8902 non-null   int64  \n",
      " 67  latitude_deg_arr                8902 non-null   float64\n",
      " 68  longitude_deg_arr               8902 non-null   float64\n",
      " 69  pop2010_arr                     8902 non-null   float64\n",
      " 70  UnemploymentRate_arr            8902 non-null   float64\n",
      " 71  holidays_arr                    8902 non-null   object \n",
      " 72  GDP_per_cap_arr                 8902 non-null   float64\n",
      " 73  closest_holidays_arr            8902 non-null   float64\n",
      " 74  distance                        8902 non-null   float64\n",
      " 75  Temp_diff                       8902 non-null   float64\n",
      " 76  Temp_diff_abs                   8902 non-null   float64\n",
      "dtypes: float64(68), int64(5), object(4)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Without External Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# features = ['WeeksToDeparture', 'oil_stock_volume',\n",
    "#                     'AAL_stock_price', 'AAL_stock_volume', 'SP_stock_price', 'SP_stock_volume',\n",
    "#                     'latitude_deg_dep', 'longitude_deg_dep', 'pop2010_dep', \n",
    "#                     'latitude_deg_arr', 'longitude_deg_arr', 'pop2010_arr', 'distance',\n",
    "#                     'LoadFactorDomestic_arr', 'PassengersDomestic_arr', 'closest_holidays_dep',\n",
    "#                     'closest_holidays_arr', 'GDP_per_cap_dep', 'GDP_per_cap_arr', 'UnemploymentRate_dep', 'UnemploymentRate_arr', \n",
    "#                     'day_departure', 'weekday_departure', 'week_departure', 'n_days_departure', 'Max TemperatureC_dep', \n",
    "#             'Max TemperatureC_arr', 'Temp_diff', 'Temp_diff_abs']\n",
    "\n",
    "# forest = RandomForestRegressor(min_samples_split=0.01, max_features=0.5, oob_score=True, verbose=0)\n",
    "# score = cross_val_score(forest, X_forest, y_train, cv=5)\n",
    "\n",
    "features_to_keep = ['WeeksToDeparture', 'week_mean', 'day_mean', 'month_mean',\n",
    "                   'route_mean', 'std_wtd', 'year_departure', 'n_days_departure']\n",
    "\n",
    "X_train_data = X_train_merged[features_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8289635460394844\n",
      "{'min_samples_split': 0.001, 'n_estimators': 100, 'oob_score': True}\n"
     ]
    }
   ],
   "source": [
    "grid_params = {\n",
    "    'n_estimators': [100],\n",
    "    'min_samples_split': [0.001],\n",
    "    'oob_score': [True]\n",
    "#     'max_features': [0.5, 0.75]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=RandomForestRegressor(),\n",
    "                  param_grid=grid_params,\n",
    "                  n_jobs=-1,\n",
    "                  cv=5,\n",
    "                  verbose=0)\n",
    "\n",
    "gs.fit(X_train_data, y_train)        \n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 0.23509879747087414 and Score: 0.9440468312147768\n",
      "Testing RMSE: 0.4267359209537187 and Score: 0.8233170990757886\n"
     ]
    }
   ],
   "source": [
    "# Prediction performance on test set is not as good as on train set\n",
    "X_test_data = X_test_merged[features_to_keep]\n",
    "\n",
    "score_train = gs.score(X_train_data, y_train)\n",
    "score_test = gs.score(X_test_data, y_test)\n",
    "\n",
    "rmse_train = mean_squared_error(y_train, gs.predict(X_train_data), squared=False)\n",
    "rmse_test = mean_squared_error(y_test, gs.predict(X_test_data), squared=False)\n",
    "\n",
    "print(f\"Training RMSE: {rmse_train} and Score: {score_train}\")\n",
    "print(f\"Testing RMSE: {rmse_test} and Score: {score_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing With External Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8547966068696906\n",
      "{'max_features': 0.75, 'min_samples_split': 0.001, 'n_estimators': 100, 'oob_score': False}\n",
      "Training RMSE: 0.20385616073781013 and Score: 0.9579300928949637\n",
      "Testing RMSE: 0.38871838814815896 and Score: 0.8533958418282023\n"
     ]
    }
   ],
   "source": [
    "features_to_keep = ['WeeksToDeparture', 'week_mean', 'day_mean', 'month_mean',\n",
    "                    'route_mean', 'std_wtd', 'year_departure', 'n_days_departure',\n",
    "                    'distance', 'closest_holidays_dep', 'closest_holidays_arr',\n",
    "                    'GDP_per_cap_dep', 'GDP_per_cap_arr',\n",
    "                    'SP_stock_volume', 'SP_stock_price',\n",
    "                    'oil_stock_price', 'oil_stock_volume',\n",
    "                    'AAL_stock_volume', 'AAL_stock_price',\n",
    "                    'Max TemperatureC_dep']\n",
    "\n",
    "X_train_data = X_train_merged[features_to_keep]\n",
    "\n",
    "grid_params = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'min_samples_split': [0.001, 0.005, 0.01],\n",
    "    'oob_score': [True, False],\n",
    "    'max_features': [0.25, 0.5, 0.75]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=RandomForestRegressor(),\n",
    "                  param_grid=grid_params,\n",
    "                  n_jobs=-1,\n",
    "                  cv=5,\n",
    "                  verbose=0)\n",
    "\n",
    "gs.fit(X_train_data, y_train)        \n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "# Prediction performance on test set is not as good as on train set\n",
    "X_test_data = X_test_merged[features_to_keep]\n",
    "\n",
    "score_train = gs.score(X_train_data, y_train)\n",
    "score_test = gs.score(X_test_data, y_test)\n",
    "\n",
    "rmse_train = mean_squared_error(y_train, gs.predict(X_train_data), squared=False)\n",
    "rmse_test = mean_squared_error(y_test, gs.predict(X_test_data), squared=False)\n",
    "\n",
    "print(f\"Training RMSE: {rmse_train} and Score: {score_train}\")\n",
    "print(f\"Testing RMSE: {rmse_test} and Score: {score_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "features_to_keep = ['WeeksToDeparture', 'week_mean', 'day_mean', 'month_mean',\n",
    "                    'route_mean', 'std_wtd', 'year_departure', 'n_days_departure',\n",
    "                    'distance', 'closest_holidays_dep', 'closest_holidays_arr',\n",
    "                    'GDP_per_cap_dep', 'GDP_per_cap_arr',\n",
    "                    'SP_stock_volume', 'SP_stock_price',\n",
    "                    'oil_stock_price', 'oil_stock_volume',\n",
    "                    'AAL_stock_volume', 'AAL_stock_price',\n",
    "                    'Max TemperatureC_dep']\n",
    "\n",
    "X_train_data = X_train_merged[features_to_keep]\n",
    "\n",
    "grid_params = {}\n",
    "\n",
    "gs = GridSearchCV(estimator=LinearRegression(),\n",
    "                  param_grid=grid_params,\n",
    "                  n_jobs=-1,\n",
    "                  cv=5,\n",
    "                  verbose=0)\n",
    "\n",
    "gs.fit(X_train_data, y_train)        \n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "# Prediction performance on test set is not as good as on train set\n",
    "X_test_data = X_test_merged[features_to_keep]\n",
    "\n",
    "score_train = gs.score(X_train_data, y_train)\n",
    "score_test = gs.score(X_test_data, y_test)\n",
    "\n",
    "rmse_train = mean_squared_error(y_train, gs.predict(X_train_data), squared=False)\n",
    "rmse_test = mean_squared_error(y_test, gs.predict(X_test_data), squared=False)\n",
    "\n",
    "print(f\"Training RMSE: {rmse_train} and Score: {score_train}\")\n",
    "print(f\"Testing RMSE: {rmse_test} and Score: {score_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PATH'] = os.environ['PATH']+';'+os.environ['CONDA_PREFIX']+r\"\\Library\\bin\\graphviz\"\n",
    "# dot_data = StringIO()\n",
    "# export_graphviz(pipeline[0].estimators_[0], out_file=dot_data)\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "# Image(graph.create_png())\n",
    "\n",
    "# # viz = dtreeviz(forest_estimator.estimators_[0], X_train_data, y_train, feature_names=X_train_data.columns, target_name=\"Target\")\n",
    "# # viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "forest_estimator = gs.best_estimator_\n",
    "pipeline = make_pipeline(forest_estimator)\n",
    "pipeline.fit(X_train_data, y_train).score(X_test_data, y_test)\n",
    "\n",
    "feature_importances = permutation_importance(\n",
    "    pipeline, X_train_data, y_train, n_repeats=10\n",
    ")\n",
    "\n",
    "sorted_idx = feature_importances.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.boxplot(feature_importances.importances[sorted_idx].T,\n",
    "           vert=False, labels=X_train_data.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (train set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = permutation_importance(\n",
    "    pipeline, X_test_data, y_test, n_repeats=10\n",
    ")\n",
    "\n",
    "sorted_idx = feature_importances.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.boxplot(feature_importances.importances[sorted_idx].T,\n",
    "           vert=False, labels=X_test_data.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(30, 30))\n",
    "# sns.heatmap(abs(Xy_train.corr()), cmap='BrBG', annot=True, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, test_features, test_labels):\n",
    "#     predictions = model.predict(test_features)\n",
    "#     errors = abs(predictions - test_labels)\n",
    "#     mape = 100 * np.mean(errors / test_labels)\n",
    "#     accuracy = 100 - mape\n",
    "#     print('Model Performance')\n",
    "# #     print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "#     print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 15))\n",
    "# heatmap = sns.heatmap(Xy_train.corr()[['Passengers']].sort_values(by='Passengers', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "# heatmap.set_title('Features Correlating with Sales Price', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# grid_params = {\n",
    "#     'n_neighbors': range(1, 200),\n",
    "#     'weights': ['uniform', 'distance'],\n",
    "#     'metric': ['euclidean', 'manhattan'],\n",
    "# }\n",
    "\n",
    "# gs = GridSearchCV(\n",
    "#     KNeighborsRegressor(),\n",
    "#     grid_params,\n",
    "#     verbose=1,\n",
    "#     cv=3,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# gs_results = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gs_results.best_score_)\n",
    "# print(gs_results.best_estimator_)\n",
    "# print(gs_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = problem.get_train_data('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# grid_params = {\n",
    "#     'randomforestregressor__n_estimators': [50, 100],\n",
    "#     'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n",
    "#     'randomforestregressor__max_depth': [4, 6, 8],\n",
    "#     'randomforestregressor__criterion': ['mse']\n",
    "# }\n",
    "\n",
    "# # pipe = make_pipeline(StandardScaler(with_mean=False), RandomForestRegressor())\n",
    "\n",
    "# gs = GridSearchCV(\n",
    "#     pipe,\n",
    "#     grid_params,\n",
    "#     verbose=1,\n",
    "#     cv=3,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# gs_results = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gs_results.best_score_)\n",
    "# print(gs_results.best_estimator_)\n",
    "# print(gs_results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.inspection import permutation_importance\n",
    "\n",
    "# feature_importances = permutation_importance(\n",
    "#     pipe, X_train, y_train, n_repeats=10\n",
    "# )\n",
    "# sorted_idx = feature_importances.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(15, 15))\n",
    "# ax.boxplot(feature_importances.importances[sorted_idx].T,\n",
    "#            vert=False, labels=X_train.columns[sorted_idx])\n",
    "# ax.set_title(\"Permutation Importances (train set)\")\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importances = permutation_importance(\n",
    "#     pipe, X_test, y_test, n_repeats=10\n",
    "# )\n",
    "# sorted_idx = feature_importances.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(15, 15))\n",
    "# ax.boxplot(feature_importances.importances[sorted_idx].T,\n",
    "#            vert=False, labels=X_test.columns[sorted_idx])\n",
    "# ax.set_title(\"Permutation Importances (test set)\")\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# X_train = sm.add_constant(X_train)\n",
    "# print(X_train.info())\n",
    "# print(\"X as array:\", np.asarray(X_train))\n",
    "# print(\"y as array:\", np.asarray(y_train))\n",
    "# model_both = forwardSelection(X_train.astype(float), y_train.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_pca(data, index, n_components=2):\n",
    "#     \"\"\" Plot the PCA transformation of Sklearn module\n",
    "\n",
    "#     Args:\n",
    "#         data (np.array): Original scaled data as numpy array \n",
    "#                          (n samples, d features)\n",
    "#         index (list): list of strings to label samples\n",
    "#         n_components (int, optional): Number of Principal Components to keep. \n",
    "#                                       Defaults to 2.\n",
    "#     \"\"\"\n",
    "\n",
    "#     pca = PCA(n_components=n_components)\n",
    "#     pca_data = pca.fit_transform(data)\n",
    "#     per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)\n",
    "\n",
    "#     labels = []\n",
    "#     for i in range(len(per_var)):\n",
    "#         labels.append('PC' + str(i + 1) + \" : \" + str(per_var[i]))\n",
    "\n",
    "#     pca_df = pd.DataFrame(pca_data, index=index, columns=labels)\n",
    "\n",
    "#     if n_components == 2:\n",
    "#         plt.scatter(pca_df[labels[0]], pca_df[labels[1]], alpha=0.1)\n",
    "#         plt.xlabel(labels[0])\n",
    "#         plt.ylabel(labels[1])\n",
    "# #         for sample in pca_df.index:\n",
    "# #             plt.annotate(sample, (pca_df.loc[sample, labels[0]], pca_df.loc[sample, labels[1]]), rotation=45)\n",
    "    \n",
    "#     elif n_components == 3:\n",
    "#         fig = plt.figure()\n",
    "#         ax = fig.add_subplot(111, projection='3d')\n",
    "#         X = pca_df[labels[0]]\n",
    "#         Y = pca_df[labels[1]]\n",
    "#         Z = pca_df[labels[2]]\n",
    "#         ax.scatter(X, Y, Z)\n",
    "#         ax.set_xlabel(labels[0], labelpad=20)\n",
    "#         ax.set_ylabel(labels[1], labelpad=20)\n",
    "#         ax.set_zlabel(labels[2], labelpad=20)\n",
    "#         for sample in pca_df.index:\n",
    "#             ax.text(pca_df.loc[sample, labels[0]], pca_df.loc[sample, labels[1]], pca_df.loc[sample, labels[2]],\n",
    "#                     '%s' % sample, size=20, color='k', rotation=50) \n",
    "#     plt.title(\"Data projected on space given by the {} principal components.\".format(n_components))\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.hist(per_var)\n",
    "#     plt.show()\n",
    "        \n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "\n",
    "# scaled_X_train_np = scaler.transform(X_train)\n",
    "# scaled_X_train_df = pd.DataFrame(scaled_X_train_np, index=X_train.index, columns=X_train.columns)\n",
    "# # scaled_consumption_df \n",
    "\n",
    "# pca = PCA(n_components=25)\n",
    "# pca_data = pca.fit_transform(scaled_X_train_np)\n",
    "# per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)\n",
    "# print(per_var)\n",
    "# print(sum(per_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# pipe = make_pipeline(OneHotEncoder(handle_unknown='ignore'),\n",
    "#                     StandardScaler(with_mean=False),\n",
    "#                     LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.fit(X_train, y_train)\n",
    "# y_train_pred = pipe.predict(X_train)\n",
    "# #print(\"10 first y_train pred = \", y_train_pred[:10])\n",
    "# #print(\"10 first y_train = \", y_train[:10])\n",
    "# print(\"Score on train set = \", pipe.score(X_train, y_train))\n",
    "\n",
    "# mean_error_train = mean_squared_error(y_train, y_train_pred)\n",
    "# print(\"Mean square error = \", mean_error_train)\n",
    "\n",
    "# y_test_pred = pipe.predict(X_test)\n",
    "# #print(\"10 first y_test pred = \", y_test_pred[:10])\n",
    "# #print(\"10 first y_test = \", y_test[:10])\n",
    "# print(\"Score on test set = \", pipe.score(X_test, y_test))\n",
    "\n",
    "# mean_error_test = mean_squared_error(y_test, y_test_pred)\n",
    "# print(\"Mean square error = \", mean_error_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
